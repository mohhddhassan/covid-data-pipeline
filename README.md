# ğŸ¦  COVID-19 Data Pipeline

This project is a simple end-to-end data pipeline that fetches global COVID-19 statistics daily from a public API and stores it in a PostgreSQL database using Apache Airflow.

---

## ğŸ“Œ Purpose

As an Associate Data Engineer intern in my learning phase, I built this mini-project to:
- Understand how data pipelines work in a real-world context
- Learn how Apache Airflow schedules and manages DAGs
- Practice database handling using PostgreSQL
- Gain familiarity with Docker-based data engineering setups

---

## âš™ï¸ Tech Stack

- **Apache Airflow**: For orchestrating and scheduling tasks
- **PostgreSQL**: As the database to store the data
- **Docker**: To containerize the entire setup
- **Python**: For fetching and inserting the data
- **disease.sh API**: Public COVID-19 stats API

---

## ğŸ“ Project Structure

covid-data-pipeline/
â”‚
â”œâ”€â”€ airflow-docker/
â”‚ â”œâ”€â”€ dags/ # Airflow DAGs
â”‚ â”œâ”€â”€ scripts/ # Python scripts for data fetching/insertion
â”‚ â”œâ”€â”€ plugins/ # (optional) custom plugins
â”‚ â”œâ”€â”€ logs/ # DAG run logs
â”‚ â”œâ”€â”€ requirements.txt # Python dependencies
â”‚ â””â”€â”€ docker-compose.yaml # Docker setup
â”‚
â”œâ”€â”€ screenshots/ # Project screenshots
â”œâ”€â”€ README.md
â””â”€â”€ .gitignore

---

---

## ğŸ› ï¸ How It Works

1. Airflow DAG runs once daily (`@daily`)
2. A Python script fetches COVID-19 stats from `https://disease.sh/v3/covid-19/all`
3. The data is inserted into a PostgreSQL table (`covid_stats`)
4. Logs are stored and monitored via Airflow UI

---

## ğŸ“¸ Screenshots

| DAG in Airflow UI | PostgreSQL Table | Logs Sample |
|------------------|------------------|-------------|
| ![DAG](./screenshots/dag_running.png) | ![Postgres](./screenshots/data_in_postgres.png) | ![Logs](./screenshots/logs_sample.png) |

---

## âœ… Learnings & Takeaways

- Set up Airflow using Docker
- Created and ran custom Python tasks in Airflow
- Connected to and inserted data into PostgreSQL using Python
- Handled task retries, error logging, and schedule intervals

---

## ğŸ§  What's Next?

This was a foundational project. As next steps, I plan to:
- Add a data deduplication logic (to avoid inserting the same row daily)
- Add a Streamlit dashboard for visualizing the data (maybe ğŸ˜‰)
- Explore parameterization and templating in Airflow

---

## ğŸ™Œ Acknowledgements

- COVID-19 data from [disease.sh](https://disease.sh/)
- Docker + Airflow base from Apache documentation

---

## ğŸ§‘â€ğŸ’» Author

**Mohamed Hussain S**  
Associate Data Engineer Intern  
[LinkedIn](https://linkedin.com/in/hussainmohhdd) | [GitHub](https://github.com/mohhddhassan)
